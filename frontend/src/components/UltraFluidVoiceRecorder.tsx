import React, { useState, useRef, useEffect, useCallback } from 'react';
import { Mic, MicOff, CheckCircle, AlertCircle, Loader2 } from 'lucide-react';

interface UltraFluidVoiceRecorderProps {
  onTranscriptionUpdate?: (text: string) => void;
  onTranscriptionComplete?: (text: string) => void;
  onError?: (error: string) => void;
  disabled?: boolean;
  className?: string;
  autoStart?: boolean;
  onListeningChange?: (isListening: boolean) => void;
  streamingMode?: boolean; // Mode streaming vers Whisper
  realTimeMode?: boolean; // Mode temps r√©el avec Web Speech API
}

export const UltraFluidVoiceRecorder: React.FC<UltraFluidVoiceRecorderProps> = ({
  onTranscriptionUpdate = () => {},
  onTranscriptionComplete = () => {},
  onError = () => {},
  disabled = false,
  className = '',
  autoStart = false,
  onListeningChange = () => {},
  streamingMode = true,
  realTimeMode = true,
}) => {
  const [isRecording, setIsRecording] = useState(false);
  const [isAvailable, setIsAvailable] = useState(true);
  const [error, setError] = useState<string | null>(null);
  const [isProcessing, setIsProcessing] = useState(false);
  const [currentTranscript, setCurrentTranscript] = useState('');
  const [confidence, setConfidence] = useState(0);
  const [latency, setLatency] = useState(0);
  
  const recognitionRef = useRef<any>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const accumulatedTextRef = useRef<string>('');
  const isRecordingRef = useRef<boolean>(false);
  const lastResultRef = useRef<string>('');
  const streamRef = useRef<MediaStream | null>(null);
  const latencyTimerRef = useRef<number>(0);

  // Appeler onListeningChange lorsque l'√©tat d'enregistrement change
  useEffect(() => {
    onListeningChange(isRecording);
  }, [isRecording, onListeningChange]);

  // Initialisation de la reconnaissance vocale native (temps r√©el)
  useEffect(() => {
    if (!realTimeMode) return;

    if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
      setIsAvailable(false);
      onError('La reconnaissance vocale native n\'est pas support√©e');
      return;
    }

    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    recognitionRef.current = new SpeechRecognition();
    
    // üöÄ PARAM√àTRES ULTRA-FLUIDES pour capture compl√®te
    recognitionRef.current.continuous = true;           // √âcoute continue OBLIGATOIRE
    recognitionRef.current.interimResults = true;       // R√©sultats interm√©diaires pour fluidit√©
    recognitionRef.current.lang = 'fr-FR';              // Fran√ßais optimis√©
    recognitionRef.current.maxAlternatives = 3;         // Plus d'alternatives pour meilleure pr√©cision
    // Pas de grammaires sp√©cifiques (comportement par d√©faut)
    
    // üéØ PARAM√àTRES AVANC√âS pour √©viter les coupures
    if ('serviceURI' in recognitionRef.current) {
      recognitionRef.current.serviceURI = null;         // Service par d√©faut (plus stable)
    }
    
    console.log('üé§ Francis Voice: Configuration ultra-fluide activ√©e');

    recognitionRef.current.onerror = (event: any) => {
      console.log('üîß Francis Voice: Gestion erreur', event.error);
      
      // üõ°Ô∏è GESTION INTELLIGENTE DES ERREURS - Ne pas arr√™ter pour des erreurs mineures
      const minorErrors = ['no-speech', 'audio-capture', 'network', 'aborted'];
      
      if (minorErrors.includes(event.error)) {
        console.log('‚ö†Ô∏è Francis Voice: Erreur mineure ignor√©e:', event.error);
        // Relancer automatiquement apr√®s erreur mineure
        if (isRecordingRef.current) {
          setTimeout(() => {
            try {
              if (recognitionRef.current && isRecordingRef.current) {
                recognitionRef.current.start();
                console.log('üîÑ Francis Voice: Relance automatique apr√®s erreur mineure');
              }
            } catch (restartError) {
              console.error('Erreur de relance:', restartError);
            }
          }, 100); // D√©lai court pour √©viter les conflits
        }
        return; // Ne pas traiter comme une vraie erreur
      }
      
      // Erreurs critiques seulement
      console.error('‚ùå Francis Voice: Erreur critique:', event.error);
      onError(`Erreur critique: ${event.error}`);
      setIsRecording(false);
      isRecordingRef.current = false;
    };

    recognitionRef.current.onend = () => {
      console.log('üîÑ Francis Voice: Session termin√©e, relance automatique...');
      
      if (isRecordingRef.current) {
        // üöÄ RELANCE ULTRA-RAPIDE avec retry intelligent
        let retryCount = 0;
        const maxRetries = 3;
        
        const attemptRestart = () => {
          try {
            if (recognitionRef.current && isRecordingRef.current) {
              recognitionRef.current.start();
              console.log(`‚úÖ Francis Voice: Relance r√©ussie (tentative ${retryCount + 1})`);
            }
          } catch (error) {
            retryCount++;
            console.warn(`‚ö†Ô∏è Francis Voice: √âchec relance (tentative ${retryCount}):`, error);
            
            if (retryCount < maxRetries && isRecordingRef.current) {
              // Retry avec d√©lai progressif
              setTimeout(attemptRestart, retryCount * 200);
            } else {
              console.error('‚ùå Francis Voice: √âchec d√©finitif de relance');
              setIsRecording(false);
              isRecordingRef.current = false;
              onError('Impossible de maintenir l\'enregistrement continu');
            }
          }
        };
        
        // D√©marrer la premi√®re tentative imm√©diatement
        setTimeout(attemptRestart, 50); // D√©lai minimal pour √©viter les conflits
      }
    };

    recognitionRef.current.onresult = (event: any) => {
      const startTime = performance.now();
      
      let interimTranscript = '';
      let finalTranscript = '';
      let maxConfidence = 0;
      let allAlternatives: string[] = [];

      // üöÄ CAPTURE ULTRA-COMPL√àTE - Analyser TOUS les r√©sultats et alternatives
      for (let i = event.resultIndex; i < event.results.length; i++) {
        const result = event.results[i];
        
        // Prendre la meilleure alternative (plus de pr√©cision)
        let bestTranscript = result[0].transcript;
        let bestConfidence = result[0].confidence || 0;
        
        // Analyser toutes les alternatives pour trouver la meilleure
        for (let j = 0; j < Math.min(result.length, 3); j++) {
          const alternative = result[j];
          const altConfidence = alternative.confidence || 0;
          const altTranscript = alternative.transcript;
          
          allAlternatives.push(altTranscript);
          
          // Si cette alternative est plus fiable, l'utiliser
          if (altConfidence > bestConfidence || (altConfidence === bestConfidence && altTranscript.length > bestTranscript.length)) {
            bestTranscript = altTranscript;
            bestConfidence = altConfidence;
          }
        }
        
        maxConfidence = Math.max(maxConfidence, bestConfidence);
        
        if (result.isFinal) {
          finalTranscript += bestTranscript + ' ';
          console.log(`‚úÖ Francis Voice: Texte final captur√©: "${bestTranscript}" (confiance: ${(bestConfidence * 100).toFixed(1)}%)`);
        } else {
          interimTranscript += bestTranscript;
          console.log(`üîÑ Francis Voice: Texte interm√©diaire: "${bestTranscript}" (confiance: ${(bestConfidence * 100).toFixed(1)}%)`);
        }
      }

      // Calcul de la latence
      const endTime = performance.now();
      const currentLatency = endTime - startTime;
      setLatency(currentLatency);
      setConfidence(maxConfidence);

      // üéØ MISE √Ä JOUR INTELLIGENTE - Toujours garder le texte le plus complet
      if (finalTranscript) {
        const newFinalText = finalTranscript.trim();
        accumulatedTextRef.current = (accumulatedTextRef.current + ' ' + newFinalText).trim();
        const currentText = accumulatedTextRef.current;
        lastResultRef.current = currentText;
        setCurrentTranscript(currentText);
        onTranscriptionUpdate(currentText);
        console.log(`üé§ Francis Voice: Texte accumul√© total: "${currentText}"`);
      } else if (interimTranscript) {
        const fullText = accumulatedTextRef.current + (accumulatedTextRef.current ? ' ' : '') + interimTranscript;
        lastResultRef.current = interimTranscript;
        setCurrentTranscript(fullText);
        onTranscriptionUpdate(fullText);
        // Ne pas logger les interm√©diaires pour √©viter le spam
      }
      
      // üìä STATISTIQUES DE CAPTURE
      if (finalTranscript || interimTranscript) {
        const totalLength = (accumulatedTextRef.current + ' ' + (finalTranscript || interimTranscript)).length;
        console.log(`üìä Francis Voice: Longueur totale captur√©e: ${totalLength} caract√®res`);
      }
    };

    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.stop();
      }
    };
  }, [realTimeMode, onError, onTranscriptionUpdate, onListeningChange]);

  // Streaming vers Whisper (mode ultra-fluide)
  const streamToWhisper = useCallback(async (audioBlob: Blob) => {
    try {
      const startTime = performance.now();
      
      // Convertir en base64
      const arrayBuffer = await audioBlob.arrayBuffer();
      const uint8Array = new Uint8Array(arrayBuffer);
      const base64Audio = btoa(String.fromCharCode.apply(null, Array.from(uint8Array)));
      
      // Envoyer √† Whisper avec optimisations
      const response = await fetch('/api/whisper/transcribe', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          audio_base64: base64Audio,
          audio_format: 'webm',
          language: 'fr',
          streaming: true, // Indicateur de streaming
        }),
      });

      if (!response.ok) {
        throw new Error(`Erreur Whisper: ${response.status}`);
      }

      const result = await response.json();
      const endTime = performance.now();
      const whisperLatency = endTime - startTime;
      
      console.log(`üé§ Whisper streaming - Latence: ${whisperLatency.toFixed(0)}ms, Confiance: ${result.language_probability || 0}`);
      
      if (result.text) {
        // Fusion intelligente avec le texte natif
        const whisperText = result.text.trim();
        const nativeText = accumulatedTextRef.current;
        
        // Si le texte Whisper est plus long, l'utiliser
        if (whisperText.length > nativeText.length * 0.8) {
          accumulatedTextRef.current = whisperText;
          setCurrentTranscript(whisperText);
          onTranscriptionUpdate(whisperText);
        }
      }

      return result;
    } catch (error) {
      console.error('Erreur streaming Whisper:', error);
      onError('Erreur de transcription Whisper');
    }
  }, [onTranscriptionUpdate, onError]);

  // Initialisation du MediaRecorder pour streaming
  useEffect(() => {
    if (!streamingMode) return;

    const initMediaRecorder = async () => {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ 
          audio: {
            // üöÄ PARAM√àTRES AUDIO ULTRA-OPTIMIS√âS pour capture compl√®te
            echoCancellation: true,           // Suppression √©cho
            noiseSuppression: false,          // D√âSACTIV√â pour garder toute la voix
            autoGainControl: true,            // Gain automatique
            sampleRate: 48000,                // Qualit√© maximale (48kHz au lieu de 16kHz)
            channelCount: 1,                  // Mono pour optimiser
            // Param√®tres optimis√©s pour capture compl√®te (param√®tres standards uniquement)
          } 
        });
        
        streamRef.current = stream;
        
        const mediaRecorder = new MediaRecorder(stream, {
          // üéØ CONFIGURATION ULTRA-FLUIDE MediaRecorder
          mimeType: 'audio/webm;codecs=opus',
          audioBitsPerSecond: 128000,           // Qualit√© √©lev√©e (128kbps au lieu de 16kbps)
          bitsPerSecond: 128000,                // D√©bit global √©lev√©
        });
        
        mediaRecorderRef.current = mediaRecorder;
        audioChunksRef.current = [];

        mediaRecorder.ondataavailable = async (event) => {
          if (event.data.size > 0) {
            audioChunksRef.current.push(event.data);
            
            // Streaming en temps r√©el vers Whisper
            if (streamingMode && isRecordingRef.current) {
              const audioBlob = new Blob([event.data], { type: 'audio/webm' });
              await streamToWhisper(audioBlob);
            }
          }
        };

        mediaRecorder.onstop = async () => {
          if (audioChunksRef.current.length > 0) {
            const finalBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' });
            await streamToWhisper(finalBlob);
          }
        };

      } catch (error) {
        console.error('Erreur initialisation MediaRecorder:', error);
        onError('Impossible d\'acc√©der au microphone');
      }
    };

    initMediaRecorder();

    return () => {
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => track.stop());
      }
    };
  }, [streamingMode, streamToWhisper, onError]);

  useEffect(() => {
    if (autoStart && isAvailable && !disabled) {
      startRecording();
    }
  }, [autoStart, isAvailable, disabled]);

  const startRecording = () => {
    if (!isAvailable || disabled) {
      setError('La reconnaissance vocale n\'est pas disponible');
      return;
    }
    
    setError(null);
    setIsProcessing(true);
    latencyTimerRef.current = performance.now();
    
    try {
      // D√©marrer la reconnaissance native
      if (realTimeMode && recognitionRef.current) {
        recognitionRef.current.start();
      }
      
      // D√©marrer le MediaRecorder pour streaming
      if (streamingMode && mediaRecorderRef.current) {
        mediaRecorderRef.current.start(1000); // Chunk toutes les secondes
      }
      
      setIsRecording(true);
      isRecordingRef.current = true;
      lastResultRef.current = '';
      setCurrentTranscript('');
      setLatency(0);
      setConfidence(0);
      
    } catch (error) {
      console.error('Erreur d√©marrage enregistrement:', error);
      const errorMsg = 'Erreur lors du d√©marrage de l\'enregistrement';
      setError(errorMsg);
      onError(errorMsg);
      setIsRecording(false);
      isRecordingRef.current = false;
      setIsProcessing(false);
    }
  };

  const stopRecording = () => {
    if (!isAvailable || !isRecording) {
      setIsProcessing(false);
      return;
    }
    
    setIsProcessing(true);
    
    try {
      // Arr√™ter la reconnaissance native
      if (recognitionRef.current) {
        recognitionRef.current.stop();
      }
      
      // Arr√™ter le MediaRecorder
      if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
        mediaRecorderRef.current.stop();
      }
      
      const finalText = accumulatedTextRef.current.trim();
      if (finalText) {
        onTranscriptionComplete(finalText);
      } else if (lastResultRef.current) {
        onTranscriptionComplete(lastResultRef.current);
      }
      
    } catch (error) {
      console.error('Erreur arr√™t enregistrement:', error);
      setError('Erreur lors de l\'arr√™t de l\'enregistrement');
    } finally {
      setIsRecording(false);
      isRecordingRef.current = false;
      setIsProcessing(false);
    }
  };

  const handleButtonClick = () => {
    if (isRecording) {
      stopRecording();
    } else {
      startRecording();
    }
  };

  if (!isAvailable) {
    return (
      <div className={`flex items-center text-[#ef4444] ${className}`}>
        <AlertCircle className="mr-2" size={16} />
        <span className="text-sm">Reconnaissance vocale non disponible</span>
      </div>
    );
  }

  return (
    <div className="relative">
      <div className="flex items-center gap-3">
        <button
          onClick={handleButtonClick}
          disabled={disabled || isProcessing}
          className={`flex-shrink-0 flex items-center justify-center w-12 h-12 rounded-full transition-all ${
            isRecording
              ? 'bg-gradient-to-br from-red-500 to-red-600 text-white shadow-lg shadow-red-500/30'
              : 'bg-gradient-to-br from-[#c5a572] to-[#e8cfa0] text-[#162238] hover:shadow-lg hover:shadow-[#c5a572]/30'
          } ${disabled || isProcessing ? 'opacity-50 cursor-not-allowed' : 'cursor-pointer hover:scale-105'} ${className}`}
          aria-label={isRecording ? 'Arr√™ter l\'enregistrement' : 'D√©marrer l\'enregistrement'}
        >
          {isProcessing && !isRecording ? (
            <Loader2 className="w-5 h-5 animate-spin" />
          ) : isRecording ? (
            <div className="flex items-center justify-center w-6 h-6">
              <div className="absolute w-4 h-4 bg-white rounded-full animate-ping"></div>
              <MicOff size={20} className="relative" />
            </div>
          ) : (
            <Mic size={20} />
          )}
        </button>
        
        <div className="flex-1">
          <div className="text-sm text-gray-300 mb-1">
            {isRecording ? 'Enregistrement ultra-fluide...' : 'Appuyez pour dicter'}
          </div>
          
          {/* Indicateurs de performance en temps r√©el */}
          {isRecording && (
            <div className="flex items-center gap-4 text-xs">
              <div className="flex items-center gap-1">
                <div className={`w-2 h-2 rounded-full ${latency < 100 ? 'bg-green-400' : latency < 300 ? 'bg-yellow-400' : 'bg-red-400'}`}></div>
                <span className="text-gray-400">Latence: {latency.toFixed(0)}ms</span>
              </div>
              
              <div className="flex items-center gap-1">
                <div className={`w-2 h-2 rounded-full ${confidence > 0.8 ? 'bg-green-400' : confidence > 0.6 ? 'bg-yellow-400' : 'bg-red-400'}`}></div>
                <span className="text-gray-400">Confiance: {(confidence * 100).toFixed(0)}%</span>
              </div>
              
              {streamingMode && (
                <div className="flex items-center gap-1">
                  <div className="w-2 h-2 rounded-full bg-blue-400 animate-pulse"></div>
                  <span className="text-gray-400">Streaming Whisper</span>
                </div>
              )}
            </div>
          )}
        </div>
      </div>
      
      {error && (
        <div className="mt-2 text-xs text-red-400 flex items-center">
          <AlertCircle className="w-3 h-3 mr-1" /> {error}
        </div>
      )}
      
      {/* Transcription cach√©e - affichage g√©r√© par le composant parent */}
      
      {isRecording && (
        <div className="absolute -top-2 -right-2 flex items-center justify-center w-6 h-6 bg-red-500 text-white text-xs font-bold rounded-full animate-pulse">
          ‚Ä¢
        </div>
      )}
    </div>
  );
};
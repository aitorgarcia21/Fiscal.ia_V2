import os
from pathlib import Path
from typing import List, Optional
import PyPDF2
import textwrap
import requests
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import urllib.parse as _uparse

"""
Extract Andorran fiscal laws (PDF) and split them into manageable text chunks
compatible avec la pipeline Mistral.

√âtapes :
1. Placer les PDF r√©cents (2025) dans backend/data/andorra_docs/
2. Ex√©cuter ce script : python extract_andorra_tax_docs.py
   ‚Üí G√©n√®re des fichiers texte dans backend/data/andorra_chunks_text/
     (andorra_chunk_0000.txt, etc.)
3. Lancer ensuite generate_andorra_embeddings.py pour cr√©er les embeddings.

Les chunks sont de 1 000 caract√®res max, sans chevauchement pour la simplicit√©.
"""

PDF_DIR = Path('data/andorra_docs')
CHUNKS_DIR = Path('data/andorra_chunks_text')
CHUNK_SIZE = 1000  # caract√®res

PDF_SOURCES = {
    "IRPF_5_2014.pdf": "https://www.impostos.ad/sites/default/files/2024-02/Llei%205_2014_IRPF.pdf",
    "IGI_95_2010.pdf": "https://www.impostos.ad/sites/default/files/2024-02/Llei%2095_2010_IGI.pdf",
    "IS_95_2014.pdf": "https://www.impostos.ad/sites/default/files/2024-02/Llei%2095_2014_ImpostSocietats.pdf",
    # ---- Imp√¥ts directs ----
    "Llei_5_2014_IRPF.pdf": "https://www.consellgeneral.ad/ca/arxiu/arxiu-de-lleis-i-textos-aprovats-en-legislatures-anteriors/vi-legislatura-2011-2015/copy_of_lleis-aprovades/llei-5-2014-del-24-d2019abril-de-l2019impost-sobre-la-renda-de-les-persones-fisiques/llei-5-2014.pdf",
    "Llei_94_2010_IRNR.pdf": "https://www.consellgeneral.ad/fitxers/documents/lleis-2010/llei-94-2010.pdf",
    "Llei_95_2010_IS.pdf": "https://www.consellgeneral.ad/fitxers/documents/lleis-2010/llei-95-2010.pdf",
    "Llei_11_2005_Rendements_Epargne.pdf": "https://www.consellgeneral.ad/fitxers/documents/lleis-2005/llei-11-2005.pdf",
    # ---- IGI ----
    "Llei_16_2004_Modifications_IGI.pdf": "https://www.consellgeneral.ad/fitxers/documents/lleis-2004/llei-16-2004.pdf",
    "Llei_11_2012_Text_refo_IGI.pdf": "https://www.consellgeneral.ad/ca/arxiu/arxiu-de-lleis-i-textos-aprovats-en-legislatures-anteriors/vi-legislatura-2011-2015/copy_of_lleis-aprovades/decret-legislatiu-del-23-07-2014-de-publicacio-del-text-refos-de-la-llei-11-2012-del-21-de-juny-de-l2019impost-general-indirecte/at_download/PDF",
    # ---- Bases fiscales ----
    "Llei_21_2014_Bases_Tributari.pdf": "https://www.consellgeneral.ad/fitxers/documents/lleis-2014/llei-21-2014.pdf",
    # ---- Administration fiscale ----
    "BOPA_61_2019_Codi_Administracio.pdf": "https://bopadocuments.blob.core.windows.net/bopa-documents/034048/pdf/CGL20220414_11_15_29.pdf",
    # ---- Conventions internationales ----
    "Convention_Andorre_France_2013.pdf": "https://www.impots.gouv.fr/sites/default/files/media/10_conventions/andorre/andorre_convention-avec-la-principaute-d-andorre-revenu-signee-le-2-avril-2013_fd_7522.pdf",
    "OCDE_MLI_Signatories.pdf": "https://www.oecd.org/content/dam/oecd/en/topics/policy-sub-issues/beps-mli/beps-mli-signatories-and-parties.pdf",
    # ---- Lois et d√©crets suppl√©mentaires (sources BOPA / Portal Jur√≠dic) ----
    # NB : certaines URL peuvent encore √©voluer ; le script tentera de les
    #      r√©cup√©rer directement et, en √©chec, s'appuiera sur crawl_bopa_for_pdfs.
    "Llei_21_2014_Bases_Ordenament_Tributari.pdf": "https://www.bopa.ad/bopa/03012015/Llei_21_2014.pdf",
    # D√©crets d'application proc√©dure fiscale 2015 (n¬∫ 11, 22, 150)
    "Decret_11_02_2015_Procediment_Tributari.pdf": "https://www.bopa.ad/bopa/11022015/Decret_11_2015.pdf",
    "Decret_22_04_2015_Procediment_Tributari.pdf": "https://www.bopa.ad/bopa/22042015/Decret_22_2015.pdf",
    "Decret_21_10_2015_Recobrament_Tributari.pdf": "https://www.bopa.ad/bopa/21102015/Decret_21_2015.pdf",
    # Plus-values immobili√®res 2017
    "Decret_6_12_2017_PlusValues_Immobilieres.pdf": "https://www.bopa.ad/bopa/06122017/Decret_PlusValues_2017.pdf",
}

BOPA_BASE = "https://www.bopa.ad"
BOPA_LEGISLACIO_URL = "https://www.bopa.ad/Legislacio"

# √âtend le filtre de mots-cl√©s pour couvrir les nouvelles r√©f√©rences
KEYWORDS = [
    'impost', 'irpf', 'igi', 'plus', 'procediment', 'recobrament',
    'tribut', 'revisio', 'reglament', 'decret', 'valor', 'base', 'renda',
    'ordenament', 'plusvalues', 'plus-v', '2015', '2017'
]

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Integration Scrapfly (scraping as-a-service)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# L'utilisateur peut fournir la cl√© via variable d'env SCRAPFLY_KEY ou param√®tre --scrapfly-key
SCRAPFLY_KEY = os.getenv("SCRAPFLY_KEY")


def scrapfly_get(url: str, render_js: bool = False, binary: bool = False) -> Optional[bytes]:
    """R√©cup√®re *url* via Scrapfly. Retourne le *bytes* du contenu (binaire ou HTML) ou None.

    ‚Ä¢ *binary* True  ‚Üí on force `format=binary` (base64) pour t√©l√©charger un PDF.
    ‚Ä¢ *render_js* True ‚Üí rend la page avec un navigateur cloud.
    """
    if not SCRAPFLY_KEY:
        return None

    base = "https://api.scrapfly.io/scrape"
    params = {
        "key": SCRAPFLY_KEY,
        "url": url,
        "asp": "true",  # bypass anti-bot si n√©cessaire
    }
    if render_js:
        params["render_js"] = "true"
    # Pour le mode binaire, Scrapfly renvoie un payload base64 dans JSON ‚Üí format=json puis d√©codage
    if binary:
        params["format"] = "json"

    api_url = f"{base}?" + _uparse.urlencode(params, safe="/:")

    try:
        r = requests.get(api_url, timeout=30)
        r.raise_for_status()
    except Exception as e:
        print(f"‚ö†Ô∏è Scrapfly request failed: {e}")
        return None

    if binary:
        try:
            payload = r.json()
            if payload["result"]["format"] != "binary":
                return None
            import base64
            return base64.b64decode(payload["result"]["content"])
        except Exception:
            return None
    else:
        # HTML/text content (result.content si success True)
        try:
            payload = r.json()
            return payload["result"]["content"].encode("utf-8")
        except Exception:
            return None


def search_pdf_duckduckgo_scrapfly(query: str, max_links: int = 10) -> List[str]:
    """Interroge DuckDuckGo via Scrapfly et renvoie max_links liens PDF.
    Avantage : permet d'obtenir des r√©sultats m√™me si DDG bloque l'IP locale.
    """
    if not SCRAPFLY_KEY:
        return []
    ddg_url = (
        "https://duckduckgo.com/html/?q=" + _uparse.quote_plus(query + " filetype:pdf")
    )
    html_bytes = scrapfly_get(ddg_url, render_js=False, binary=False)
    if not html_bytes:
        return []
    import re

    html = html_bytes.decode("utf-8", errors="ignore")
    pdf_links = re.findall(r"https?://[^\"']+\.pdf", html, flags=re.I)
    # Nettoyage
    seen, clean = set(), []
    for link in pdf_links:
        if link not in seen:
            seen.add(link)
            clean.append(link)
        if len(clean) >= max_links:
            break
    return clean


def extract_text_from_pdf(pdf_path: Path) -> str:
    """Return full extracted text from a PDF (UTF-8)."""
    text = ""
    try:
        with pdf_path.open('rb') as f:
            reader = PyPDF2.PdfReader(f)
            for page in reader.pages:
                text += page.extract_text() or ""
    except Exception as e:
        print(f"‚ùå Erreur extraction {pdf_path.name}: {e}")
    return text


def chunk_text(text: str, size: int = CHUNK_SIZE) -> List[str]:
    """Split text into chunks of ~size characters without breaking words."""
    # textwrap.wrap conserve les mots entiers
    return textwrap.wrap(text, width=size, break_long_words=False, break_on_hyphens=False)


def save_chunks(chunks: List[str]):
    CHUNKS_DIR.mkdir(parents=True, exist_ok=True)
    existing = list(CHUNKS_DIR.glob('andorra_chunk_*.txt'))
    start_idx = len(existing)
    for i, chunk in enumerate(chunks, start=start_idx):
        filename = CHUNKS_DIR / f"andorra_chunk_{i:04d}.txt"
        filename.write_text(chunk, encoding='utf-8')


def attempt_fallback_download(url: str, dest: Path) -> bool:
    """Essaye de retrouver le PDF via diverses strat√©gies (Scrapfly & DuckDuckGo)."""
    # 1Ô∏è‚É£ Scrapfly : tentative de t√©l√©chargement direct (binaire)
    if SCRAPFLY_KEY:
        binary = scrapfly_get(url, binary=True)
        if binary:
            dest.write_bytes(binary)
            print(f"   ‚Üí PDF r√©cup√©r√© via Scrapfly ({len(binary)} o).")
            return True

    # 2Ô∏è‚É£ Recherche DuckDuckGo via Scrapfly
    query = Path(url).stem.replace("_", " ")
    candidates = search_pdf_duckduckgo_scrapfly(query)
    for link in candidates:
        print(f"   ‚Üí Test candidat DuckDuckGo : {link}")
        if SCRAPFLY_KEY:
            content = scrapfly_get(link, binary=True)
            if content:
                dest.write_bytes(content)
                print("      ‚úì t√©l√©charg√© (Scrapfly)")
                return True
        # Fallback local (sans Scrapfly)
        try:
            r = requests.get(link, timeout=15, stream=True)
            if r.status_code == 200 and r.headers.get("content-type", "").startswith("application/pdf"):
                dest.write_bytes(r.content)
                print("      ‚úì t√©l√©charg√© (direct)")
                return True
        except Exception:
            continue

    return False


def download_pdfs():
    PDF_DIR.mkdir(parents=True, exist_ok=True)
    for filename, url in PDF_SOURCES.items():
        dest = PDF_DIR / filename
        if dest.exists():
            continue
        try:
            print(f"‚¨áÔ∏è T√©l√©chargement de {url} ‚Ä¶")
            r = requests.get(url, timeout=30)
            if r.status_code == 200 and r.headers.get('content-type', '').startswith('application/pdf'):
                dest.write_bytes(r.content)
                print(f"   ‚Üí Enregistr√© sous {dest}")
            else:
                # Essayer un fallback : scruter le r√©pertoire parent ou le site du Consell General
                print(f"‚ö†Ô∏è √âchec t√©l√©chargement direct (code {r.status_code}). Tentative de d√©couverte‚Ä¶")
                if attempt_fallback_download(url, dest):
                    print(f"   ‚Üí PDF r√©cup√©r√© via fallback et enregistr√© sous {dest}")
                else:
                    print(f"‚ùå Impossible de r√©cup√©rer {filename}")
        except Exception as e:
            print(f"‚ùå Erreur t√©l√©chargement {url}: {e}")


def crawl_bopa_for_pdfs(max_pdfs: int = 20):
    """Parcourt la page l√©gislation et r√©cup√®re les liens PDF (lois)."""
    try:
        resp = requests.get(BOPA_LEGISLACIO_URL, timeout=30)
        if resp.status_code != 200:
            print(f"‚ö†Ô∏è Impossible d'acc√©der √† {BOPA_LEGISLACIO_URL} (code {resp.status_code})")
            return
        soup = BeautifulSoup(resp.text, 'html.parser')
        links = [a.get('href') for a in soup.find_all('a', href=True) if a['href'].lower().endswith('.pdf')]
        found = 0
        for href in links:
            if found >= max_pdfs:
                break
            full_url = urljoin(BOPA_BASE, href) if urlparse(href).netloc == '' else href
            filename = Path(urlparse(full_url).path).name
            if not filename:
                continue
            # Filtrer uniquement documents fiscaux (lois ou d√©crets)
            if not any(kw in filename.lower() for kw in KEYWORDS):
                continue
            if (PDF_DIR / filename).exists():
                continue
            try:
                print(f"‚¨áÔ∏è T√©l√©chargement BOPA {full_url} ‚Ä¶")
                r = requests.get(full_url, timeout=30, stream=True)
                if r.status_code == 200:
                    (PDF_DIR / filename).write_bytes(r.content)
                    print(f"   ‚Üí {filename} sauvegard√©")
                    found += 1
                else:
                    print(f"   ‚ö†Ô∏è code {r.status_code} pour {full_url}")
            except Exception as e:
                print(f"‚ùå Erreur t√©l√©chargement {full_url}: {e}")
    except Exception as e:
        print(f"‚ùå Erreur lors du crawl BOPA: {e}")


def process_pdfs(target_files: List[Path]):
    """Extrait le texte et enregistre les chunks pour chaque PDF fourni."""
    all_chunks: List[str] = []
    for pdf_file in target_files:
        if not pdf_file.exists():
            print(f"‚ö†Ô∏è Fichier introuvable : {pdf_file}")
            continue
        print(f"üìÑ Extraction de {pdf_file.name}‚Ä¶")
        text = extract_text_from_pdf(pdf_file)
        if not text.strip():
            print(f"‚ö†Ô∏è Texte vide pour {pdf_file.name}, ignor√©.")
            continue
        chunks = chunk_text(text)
        print(f"   ‚Üí {len(chunks)} chunks cr√©√©s")
        all_chunks.extend(chunks)
    if all_chunks:
        save_chunks(all_chunks)
        print(f"‚úÖ {len(all_chunks)} chunks enregistr√©s dans {CHUNKS_DIR}")
    else:
        print("‚ö†Ô∏è Aucun chunk g√©n√©r√©.")


def main():
    import argparse
    parser = argparse.ArgumentParser(description="Extraction PDF lois fiscales andorranes ‚Üí chunks texte")
    parser.add_argument("--file", help="Nom du PDF cible d√©j√† pr√©sent ou √† t√©l√©charger")
    parser.add_argument("--url", help="URL √† tenter avant fallback")
    parser.add_argument("--scrapfly-key", help="Cl√© API Scrapfly (sinon SCRAPFLY_KEY env)")
    parser.add_argument("--skip-crawl", action="store_true", help="Ne pas crawler le BOPA pour des PDF suppl√©mentaires")
    args = parser.parse_args()

    global SCRAPFLY_KEY
    if args.scrapfly_key:
        SCRAPFLY_KEY = args.scrapfly_key

    PDF_DIR.mkdir(parents=True, exist_ok=True)

    if args.file:
        # D√©termination du chemin complet
        pdf_path = Path(args.file)
        if not pdf_path.is_absolute():
            pdf_path = PDF_DIR / pdf_path.name

        # T√©l√©charger depuis l'URL fournie si besoin
        if not pdf_path.exists() and args.url:
            try:
                print(f"‚¨áÔ∏è T√©l√©chargement de {args.url}‚Ä¶")
                r = requests.get(args.url, timeout=30)
                if r.status_code == 200 and r.headers.get('content-type', '').startswith('application/pdf'):
                    pdf_path.write_bytes(r.content)
                    print(f"   ‚Üí PDF enregistr√© sous {pdf_path}")
                else:
                    print(f"‚ö†Ô∏è Impossible de t√©l√©charger {args.url} (code {r.status_code})")
                    # Essayer fallback Scrapfly / DuckDuckGo
                    if attempt_fallback_download(args.url, pdf_path):
                        print(f"   ‚Üí PDF r√©cup√©r√© via fallback et enregistr√© sous {pdf_path}")
            except Exception as e:
                print(f"‚ùå Erreur t√©l√©chargement {args.url}: {e}")

        process_pdfs([pdf_path])
    else:
        # Mode complet (tous les PDF connus + crawl)
        download_pdfs()
        if not args.skip_crawl:
            crawl_bopa_for_pdfs(max_pdfs=50)

        pdf_files = list(PDF_DIR.glob('*.pdf'))
        if not pdf_files:
            print(f"‚ö†Ô∏è Aucun PDF trouv√© dans {PDF_DIR}. Placez-y les lois andorranes puis r√©ex√©cutez.")
            return
        process_pdfs(pdf_files)

    # √âtape finale : g√©n√©ration (ou mise √† jour) des embeddings
    try:
        from backend.generate_andorra_embeddings import generate_embeddings
        generate_embeddings()
    except Exception as e:
        print(f"‚ö†Ô∏è Embeddings non g√©n√©r√©s automatiquement : {e}")


if __name__ == '__main__':
    main() 
#!/bin/bash

echo "üöÄ D√©marrage de Fiscal.ia avec Ollama..."

# Installer Ollama si possible sur Railway
if command -v curl > /dev/null 2>&1; then
    echo "üì¶ Installation d'Ollama..."
    curl -fsSL https://ollama.ai/install.sh | sh 2>/dev/null || echo "‚ö†Ô∏è Ollama non installable (environnement Railway restreint)"
fi

# D√©marrer Ollama si disponible
if command -v ollama > /dev/null 2>&1; then
    echo "ü§ñ D√©marrage d'Ollama en arri√®re-plan..."
    ollama serve > /dev/null 2>&1 &
    OLLAMA_PID=$!
    
    # Attendre qu'Ollama soit pr√™t
    echo "‚è≥ Attente du d√©marrage d'Ollama..."
    sleep 10
    
    # T√©l√©charger un mod√®le l√©ger pour Railway
    echo "üì• T√©l√©chargement du mod√®le Mistral 7B..."
    ollama pull mistral:7b-instruct-q4_0 2>/dev/null || echo "‚ö†Ô∏è Mod√®le non t√©l√©chargeable"
    
    export LLM_ENDPOINT="http://localhost:11434"
    echo "‚úÖ Ollama configur√© sur $LLM_ENDPOINT"
else
    echo "‚ö†Ô∏è Ollama non disponible - Francis Particulier utilisera le fallback regex"
fi

# V√©rifier si les embeddings existent
if [ ! -d "data/embeddings" ] || [ -z "$(ls -A data/embeddings 2>/dev/null)" ]; then
    echo "‚ö†Ô∏è Embeddings CGI manquants, g√©n√©ration en cours..."
    python -c "
from mistral_embeddings import generate_all_embeddings
print('G√©n√©ration des embeddings CGI...')
generate_all_embeddings()
print('‚úÖ Embeddings CGI g√©n√©r√©s')
"
fi

# V√©rifier si les chunks CGI existent
if [ ! -d "data/cgi_chunks" ] || [ -z "$(ls -A data/cgi_chunks 2>/dev/null)" ]; then
    echo "‚ö†Ô∏è Chunks CGI manquants, extraction en cours..."
    python extract_cgi_articles.py
fi

# V√©rifier si les embeddings BOFiP existent
if [ ! -d "data/bofip_embeddings" ] || [ -z "$(ls -A data/bofip_embeddings 2>/dev/null)" ]; then
    echo "‚ö†Ô∏è Embeddings BOFiP manquants, g√©n√©ration en cours..."
    python generate_bofip_embeddings.py
fi

echo "‚úÖ V√©rification des donn√©es termin√©e"

# D√©marrer l'application
exec uvicorn main:app --host 0.0.0.0 --port $PORT 
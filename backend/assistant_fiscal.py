import os
import json
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple
import requests
import time
import signal
from mistralai.client import MistralClient
from mistralai.models.chat_completion import ChatMessage

# Importer les nouveaux modules RAG
from mistral_embeddings import search_similar_bofip_chunks, get_embedding as get_embedding_from_mistral_script, cosine_similarity as cosine_similarity_from_mistral_script
from rag_cgi import get_cgi_response
from mistral_cgi_embeddings import load_embeddings, search_similar_articles

# Configuration
MISTRAL_API_KEY = os.getenv("MISTRAL_API_KEY")

# Pour les articles CGI (existants)
CGI_EMBEDDINGS_DIR = Path("data/embeddings") # Utiliser Path pour la coh√©rence
CGI_CHUNKS_DIR = Path("data/cgi_chunks")

# Pour les chunks BOFIP (nouveaux)
# Ces chemins sont d√©j√† utilis√©s par search_similar_bofip_chunks, mais les red√©finir ici peut √™tre utile pour la clart√©
# ou si on voulait acc√©der aux fichiers directement depuis ce script √† l'avenir.
BOFIP_CHUNKS_TEXT_DIR = Path("data/bofip_chunks_text")
BOFIP_EMBEDDINGS_DIR = Path("data/bofip_embeddings")


# Initialisation du client Mistral
client = MistralClient(api_key=MISTRAL_API_KEY) if MISTRAL_API_KEY else None

# Utiliser les fonctions get_embedding et cosine_similarity du script mistral_embeddings
# pour √©viter la red√©finition et assurer la coh√©rence.
# Si get_embedding est sp√©cifique √† ce fichier (par exemple, gestion d'erreur diff√©rente), 
# il faudrait le clarifier ou le fusionner.
# Pour l'instant, supposons que celle de mistral_embeddings.py est la r√©f√©rence.
get_embedding = get_embedding_from_mistral_script
cosine_similarity = cosine_similarity_from_mistral_script


def format_article_for_display(article_data: Dict) -> str:
    """Formate un article du CGI pour l'affichage avec sa structure."""
    chunks = article_data.get('chunks', [])
    if not chunks:
        return f"Article {article_data.get('article_number', 'N/A')} (pas de contenu)"
    
    return f"Article {article_data.get('article_number', 'N/A')}\n\n" + '\n\n'.join(chunks)

def search_similar_cgi_articles(query: str, top_k: int = 3) -> List[Dict]:
    """Recherche les articles du CGI les plus similaires √† la requ√™te avec le nouveau syst√®me."""
    try:
        embeddings = load_embeddings()
        if not embeddings:
            return []
        
        similar_articles_raw = search_similar_articles(query, embeddings, top_k=top_k*2) # Chercher plus d'articles
        
        # Filtrer les articles CGI peu pertinents (similarit√© < 0.7)
        # Note: search_similar_articles dans mistral_cgi_embeddings.py retourne d√©j√† (article_data, similarity)
        # Nous devons ajuster la fa√ßon dont nous acc√©dons √† la similarit√© ici.
        # Supposons que search_similar_articles retourne une liste de tuples (article_dict, similarity_score)
        
        filtered_articles = []
        for article_tuple in similar_articles_raw:
            if isinstance(article_tuple, tuple) and len(article_tuple) == 2:
                article_data, similarity_score = article_tuple
                if similarity_score >= 0.7:
                    filtered_articles.append(article_data) # Garder seulement les donn√©es de l'article
            else:
                # G√©rer le cas o√π le format n'est pas celui attendu, pour √©viter les erreurs
                # On pourrait logger un avertissement ici
                # Pour l'instant, on ignore cet √©l√©ment si le format est incorrect
                pass 

        # Formater pour la compatibilit√© avec le reste du code
        results = []
        # Prendre seulement top_k articles APR√àS filtrage
        for article_data in filtered_articles[:top_k]: 
            results.append({
                # 'similarity': similarity_score, # Utiliser la vraie similarit√© si besoin plus tard
                'content': article_data.get('text', ''),
                'source': f"CGI Article {article_data.get('article_number', 'N/A')}",
                'article_id': article_data.get('article_number', 'N/A')
            })
        
        return results
    except Exception as e:
        return []

def create_prompt(query: str, cgi_articles: List[Dict], bofip_chunks: List[Dict], conversation_history: List[Dict] = None) -> str:
    """Cr√©e le prompt pour l'assistant fiscal avec contextes CGI et BOFIP et m√©moire de conversation."""
    
    cgi_context_str = "N/A"
    if cgi_articles:
        cgi_context_str = "\n\n".join([
            f"Source: {article.get('source', 'CGI N/A')}\nContenu: {article['content']}"
            for article in cgi_articles
        ])

    bofip_context_str = "N/A"
    if bofip_chunks:
        bofip_context_str = "\n\n".join([
            f"Source: BOFIP (fichier: {chunk.get('file', 'N/A')}, similarit√©: {chunk.get('similarity', 0):.3f})\nContenu: {chunk['text']}"
            for chunk in bofip_chunks
        ])
    
    # Construire l'historique de conversation
    history_str = "N/A"
    if conversation_history and len(conversation_history) > 1:
        history_items = []
        for msg in conversation_history[-6:]:  # Garder les 6 derniers messages maximum
            role = "Utilisateur" if msg.get('role') == 'user' else "Francis"
            content = msg.get('content', '')[:200] + "..." if len(msg.get('content', '')) > 200 else msg.get('content', '')
            history_items.append(f"{role}: {content}")
        history_str = "\n".join(history_items)
    
    prompt = f"""Tu es Francis, un assistant fiscal expert hautement sp√©cialis√© en fiscalit√© fran√ßaise, cr√©√© par la soci√©t√© Fiscal.ia. Tu es l'accompagnateur fiscal personnel de l'utilisateur.

Historique de conversation r√©cente:
---
{history_str}
---

Question actuelle: {query}

Contexte du Code G√©n√©ral des Imp√¥ts (CGI):
---
{cgi_context_str}
---

Contexte du BOFIP:
---
{bofip_context_str}
---

Instructions:
- Tu es un expert fiscal et tu DOIS aider l'utilisateur en r√©pondant √† sa question de mani√®re claire et pratique.
- R√àGLE ABSOLUE : N'utilise JAMAIS d'ast√©risques (*), de formatage markdown, de gras, d'italique ou de caract√®res sp√©ciaux de mise en forme dans tes r√©ponses.
- √âcris EXCLUSIVEMENT en texte simple, clair et lisible.
- Utilise uniquement des mots, des chiffres, des points, des virgules et des tirets simples pour structurer.
- INTERDIT : *, **, ___, `, #, etc.
- AUTORIS√â : texte simple avec num√©rotation (1., 2., 3.) et tirets (-) pour les listes.

üéØ ACCOMPAGNEMENT FISCAL PROACTIF :
- M√âMOIRE : Utilise l'historique de conversation pour comprendre le contexte et la situation de l'utilisateur
- SUIVI : Fais r√©f√©rence aux √©changes pr√©c√©dents quand c'est pertinent
- PROACTIVIT√â : Propose des actions concr√®tes, des optimisations et des conseils personnalis√©s
- √âCH√âANCES : Rappelle les dates importantes (d√©clarations, versements, etc.)
- ANTICIPATION : Identifie les opportunit√©s d'optimisation fiscale
- QUESTIONS DE SUIVI : Pose des questions pertinentes pour mieux accompagner l'utilisateur

üéØ R√àGLES STRICTES DE PR√âCISION FISCALE :
- INTERDIT ABSOLU : Inventer des pourcentages, taux ou seuils non mentionn√©s dans les sources CGI/BOFIP
- OBLIGATION : Utiliser UNIQUEMENT les chiffres pr√©sents dans les textes officiels fournis
- En cas de doute sur un pourcentage : dire "je dois v√©rifier cette information pr√©cise"
- JAMAIS d'approximation sur les taux d'imposition, seuils de d√©tention, dur√©es l√©gales
- Si les sources ne contiennent pas l'information exacte : le pr√©ciser clairement

üóìÔ∏è ANN√âE FISCALE 2025 :
- Donne TOUJOURS les informations pour 2025 sauf mention contraire
- Utilise prioritairement les donn√©es du CGI et BOFIP pour les bar√®mes officiels 2025

üìù MA√éTRISE PARFAITE DES ACRONYMES FISCAUX :
- TMI = Tranche Marginale d'Imposition | IFI = Imp√¥t Fortune Immobili√®re | TVA = Taxe Valeur Ajout√©e
- IR = Imp√¥t Revenu | IS = Imp√¥t Soci√©t√©s | CFE = Cotisation Fonci√®re Entreprises
- BIC = B√©n√©fices Industriels Commerciaux | BNC = B√©n√©fices Non Commerciaux | BA = B√©n√©fices Agricoles
- PEA = Plan √âpargne Actions | PER = Plan √âpargne Retraite | PERP = Plan √âpargne Retraite Populaire
- LMNP = Location Meubl√©e Non Professionnelle | LMP = Location Meubl√©e Professionnelle
- SCPI = Soci√©t√© Civile Placement Immobilier | SCI = Soci√©t√© Civile Immobili√®re
- SASU = Soci√©t√© par Actions Simplifi√©e Unipersonnelle | EURL = Entreprise Unipersonnelle Responsabilit√© Limit√©e
- SARL = Soci√©t√© Responsabilit√© Limit√©e | SAS = Soci√©t√© par Actions Simplifi√©e
- EIRL = Entreprise Individuelle Responsabilit√© Limit√©e | EI = Entreprise Individuelle
- RSI = R√©gime Social Ind√©pendants | URSSAF = Union Recouvrement S√©curit√© Sociale
- CAF = Caisse Allocations Familiales | MSA = Mutualit√© Sociale Agricole
- DGFIP = Direction G√©n√©rale Finances Publiques | SIP = Service Imp√¥ts Particuliers
- SIE = Service Imp√¥ts Entreprises | CDI = Centre Des Imp√¥ts
- BOFIP = Bulletin Officiel Finances Publiques | CGI = Code G√©n√©ral Imp√¥ts
- LF = Loi Finances | PLF = Projet Loi Finances | LFSS = Loi Financement S√©curit√© Sociale
- CIR = Cr√©dit Imp√¥t Recherche | CICE = Cr√©dit Imp√¥t Comp√©titivit√© Emploi
- CIMR = Contribution Institutions M√©dicales Retirement | C3S = Contribution Sociale Solidarit√©
- CRDS = Contribution Remboursement Dette Sociale | CSG = Contribution Sociale G√©n√©ralis√©e
- CVAE = Cotisation Valeur Ajout√©e Entreprises | CET = Contribution √âconomique Territoriale
- DMTO = Droits Mutation Titre On√©reux | DMTG = Droits Mutation Titre Gratuit
- TEOM = Taxe Enl√®vement Ordures M√©nag√®res | TH = Taxe Habitation
- TFPB = Taxe Fonci√®re Propri√©t√©s B√¢ties | TFPNB = Taxe Fonci√®re Propri√©t√©s Non B√¢ties

üéØ EXPERTISE TECHNIQUE COMPL√àTE :
- R√©gimes fiscaux : Micro, R√©el simplifi√©, R√©el normal, D√©claration contr√¥l√©e
- Niches fiscales : Pinel, Denormandie, Malraux, FCPI, FIP, Girardin
- Plus-values : Immobili√®res, mobili√®res, professionnelles, abattements 2025
- Transmission : Donation, succession, d√©membrement, pacte Dutreil
- International : CUF, conventions, exit tax, revenus √©trangers
- Contr√¥les : ESFP, v√©rification comptabilit√©, contr√¥le sur pi√®ces

üìà ACCOMPAGNEMENT PERSONNALIS√â :
- Analyse la situation compl√®te de l'utilisateur
- Propose des optimisations concr√®tes et chiffr√©es
- Sugg√®re des actions √† court et long terme
- Rappelle les √©ch√©ances importantes
- Anticipe les besoins fiscaux futurs
- Pose des questions de suivi pertinentes

- Utilise prioritairement les informations fournies du CGI et du BOFIP pour construire ta r√©ponse.
- Si les contextes contiennent des √©l√©ments pertinents, m√™me partiels, utilise-les pour donner une r√©ponse utile.
- Synth√©tise les informations disponibles et donne des conseils pratiques.
- Tu peux mentionner tes sources (ex: "Selon l'article X du CGI" ou "D'apr√®s le BOFIP") mais ce n'est pas obligatoire.
- √âVITE les formules trop formelles comme "En tant que Francis..." ou "les informations ne me permettent pas".
- Sois direct, utile et professionnel.
- Si tu n'as vraiment aucune information pertinente, donne quand m√™me une r√©ponse g√©n√©rale bas√©e sur tes connaissances fiscales fran√ßaises pour 2025.

COMPORTEMENT D'ACCOMPAGNEMENT :
- Si c'est une nouvelle conversation, pr√©sente-toi bri√®vement et demande la situation de l'utilisateur
- Si l'utilisateur a d√©j√† partag√© des informations, fais-y r√©f√©rence et propose des conseils personnalis√©s
- Termine toujours par une question de suivi ou une proposition d'action concr√®te
- Sois proactif et bienveillant dans tes conseils

R√©ponds directement √† la question:"""
    
    return prompt

def get_fiscal_response(query: str, conversation_history: List[Dict] = None) -> Tuple[str, List[str], float]:
    """Obtient une r√©ponse de l'assistant fiscal et les sources utilis√©es avec m√©moire de conversation.
    Version optimis√©e Railway avec timeouts stricts et fallbacks intelligents."""
    all_sources_for_api = []
    confidence_score = 0.5 
    
    # Variables pour les sources trouv√©es
    similar_cgi_articles = []
    similar_bofip_chunks = []

    try:
        if not client:
            return "Erreur: Client Mistral non configur√©", [], 0.0
        
        # === PHASE 1: RECHERCHE CGI AVEC TIMEOUT ===
        try:
            import signal
            
            def timeout_handler(signum, frame):
                raise TimeoutError("CGI search timeout")
            
            # Timeout de 3 secondes pour la recherche CGI
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(3)
            
            similar_cgi_articles = search_similar_cgi_articles(query, top_k=2)
            signal.alarm(0)  # Annuler le timeout
            
            if similar_cgi_articles:
                all_sources_for_api.extend([art.get('source', 'CGI inconnu') for art in similar_cgi_articles])
                confidence_score = max(confidence_score, 0.7)
                
        except (TimeoutError, Exception) as e:
            # Fallback: pas de recherche CGI, on continue
            similar_cgi_articles = []
            # print(f"[FALLBACK] CGI search failed: {e}")
        
        # === PHASE 2: RECHERCHE BOFIP AVEC TIMEOUT ===
        try:
            # Timeout de 3 secondes pour la recherche BOFIP
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(3)
            
            similar_bofip_chunks_raw = search_similar_bofip_chunks(query, top_k=4)
            similar_bofip_chunks = [c for c in similar_bofip_chunks_raw if c.get('similarity', 0) >= 0.6][:2]
            signal.alarm(0)  # Annuler le timeout
            
            if similar_bofip_chunks:
                all_sources_for_api.extend([f"BOFIP: {chunk.get('file', 'Chunk inconnu')}" for chunk in similar_bofip_chunks])
                confidence_score = max(confidence_score, 0.75)
                
        except (TimeoutError, Exception) as e:
            # Fallback: pas de recherche BOFIP, on continue
            similar_bofip_chunks = []
            # print(f"[FALLBACK] BOFIP search failed: {e}")

        # === PHASE 3: FALLBACK INTELLIGENT SI PAS DE SOURCES RAG ===
        if not similar_cgi_articles and not similar_bofip_chunks:
            # Utiliser des connaissances g√©n√©rales bas√©es sur des mots-cl√©s
            query_lower = query.lower()
            
            # D√©tecter le contexte fiscal pour une r√©ponse cibl√©e
            if any(word in query_lower for word in ['tmi', 'tranche', 'marginal', 'imposition']):
                context = "Les TMI 2025 sont : 0%, 11%, 30%, 41%, 45%"
                all_sources_for_api.append("Bar√®me fiscal 2025")
                confidence_score = 0.8
            elif any(word in query_lower for word in ['pea', 'plan', '√©pargne', 'actions']):
                context = "Le PEA permet d'investir en actions europ√©ennes avec exon√©ration fiscale apr√®s 5 ans"
                all_sources_for_api.append("Code fiscal - PEA")
                confidence_score = 0.8
            elif any(word in query_lower for word in ['plus-value', 'immobilier', 'r√©sidence principale']):
                context = "La r√©sidence principale est exon√©r√©e de plus-values. Pour les autres biens, abattement selon dur√©e de d√©tention"
                all_sources_for_api.append("CGI - Plus-values immobili√®res")
                confidence_score = 0.8
            elif any(word in query_lower for word in ['micro', 'entrepreneur', 'r√©gime']):
                context = "Le r√©gime micro vous permet de d√©clarer uniquement votre chiffre d'affaires avec un abattement forfaitaire"
                all_sources_for_api.append("CGI - R√©gimes fiscaux")
                confidence_score = 0.8
            else:
                # Contexte g√©n√©ral
                context = "En tant qu'expert fiscal, je vais analyser votre situation"
                all_sources_for_api.append("Expertise Francis")
                confidence_score = 0.6
        else:
            context = None  # Utiliser le RAG normal

        # === PHASE 4: G√âN√âRATION DE R√âPONSE AVEC TIMEOUT ===
        try:
            if context:
                # Prompt simplifi√© avec contexte de fallback
                prompt = f"""Tu es Francis, assistant fiscal expert de Fiscal.ia.

Question: {query}

Contexte fiscal disponible: {context}

Historique r√©cent: {conversation_history[-2:] if conversation_history else "Nouvelle conversation"}

R√©ponds de mani√®re claire et pratique en tant qu'expert fiscal, sans formatage markdown.
Si tu n'as pas assez d'informations pr√©cises, explique ce qu'il faudrait savoir pour donner une r√©ponse compl√®te."""
            else:
                # Prompt complet avec RAG
                prompt = create_prompt(query, similar_cgi_articles, similar_bofip_chunks, conversation_history)
            
            # Timeout de 8 secondes pour l'API Mistral
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(8)
            
            messages = [ChatMessage(role="user", content=prompt)]
            
            chat_response = client.chat(
                model="mistral-large-latest",
                messages=messages,
                temperature=0.5,
                max_tokens=1200  # R√©duit pour √™tre plus rapide
            )
            
            signal.alarm(0)  # Annuler le timeout
            answer = chat_response.choices[0].message.content

            # Ajuster la confiance selon la r√©ponse
            if "ne me permettent pas de r√©pondre" in answer or "pas d'informations suffisantes" in answer:
                confidence_score = min(confidence_score, 0.4)
            elif similar_cgi_articles or similar_bofip_chunks:
                confidence_score = 0.85

            return answer, list(set(all_sources_for_api)), confidence_score
            
        except (TimeoutError, Exception) as e:
            # Fallback ultime: r√©ponse pr√©-g√©n√©r√©e intelligente
            fallback_responses = {
                'tmi': "Votre TMI d√©pend de vos revenus 2025. Les tranches sont : 0% jusqu'√† 11 294‚Ç¨, 11% jusqu'√† 28 797‚Ç¨, 30% jusqu'√† 82 341‚Ç¨, 41% jusqu'√† 177 106‚Ç¨, 45% au-del√†. Voulez-vous que je calcule votre TMI exacte ?",
                'pea': "Le PEA vous permet d'investir jusqu'√† 150 000‚Ç¨ en actions europ√©ennes. Les gains sont exon√©r√©s d'imp√¥t apr√®s 5 ans de d√©tention. Souhaitez-vous des pr√©cisions sur les conditions ?",
                'plus-value': "Les plus-values immobili√®res b√©n√©ficient d'abattements selon la dur√©e de d√©tention. Votre r√©sidence principale est totalement exon√©r√©e. Avez-vous un projet de vente sp√©cifique ?",
                'micro': "Le r√©gime micro vous permet de d√©clarer uniquement votre chiffre d'affaires avec un abattement forfaitaire. Quel est votre domaine d'activit√© pour vous conseiller pr√©cis√©ment ?",
            }
            
            query_lower = query.lower()
            for keyword, response in fallback_responses.items():
                if keyword in query_lower:
                    return response, ["Expertise Francis"], 0.7
            
            # R√©ponse g√©n√©rale de fallback
            return f"Je vais analyser votre question fiscale sur '{query}'. Pour vous donner une r√©ponse pr√©cise et personnalis√©e, pouvez-vous me pr√©ciser votre situation (salari√©, entrepreneur, investisseur) et votre objectif ?", ["Expert Francis"], 0.6

    except Exception as e:
        return "Je rencontre des difficult√©s techniques temporaires. Pouvez-vous reformuler votre question fiscale ?", [], 0.1

def get_fiscal_response_stream(query: str, conversation_history: List[Dict] = None):
    """Version streaming de get_fiscal_response qui envoie la r√©ponse chunk par chunk.
    Version optimis√©e Railway avec timeouts et fallbacks."""
    try:
        # Envoyer le statut initial
        yield json.dumps({
            "type": "status",
            "message": "üîç Initialisation de Francis...",
            "progress": 10
        }) + "\n"
        
        if not client:
            yield json.dumps({
                "type": "error",
                "message": "Erreur: Client Mistral non configur√©"
            }) + "\n"
            return

        # Variables pour les sources
        similar_cgi_articles = []
        similar_bofip_chunks = []
        all_sources = []
        confidence_score = 0.5

        # === RECHERCHE CGI AVEC TIMEOUT ===
        yield json.dumps({
            "type": "status", 
            "message": "üìñ Recherche dans le CGI...",
            "progress": 25
        }) + "\n"
        
        try:
            import signal
            
            def timeout_handler(signum, frame):
                raise TimeoutError("Search timeout")
            
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(3)
            
            similar_cgi_articles = search_similar_cgi_articles(query, top_k=2)
            signal.alarm(0)
            
            if similar_cgi_articles:
                all_sources.extend([art.get('source', 'CGI inconnu') for art in similar_cgi_articles])
                confidence_score = max(confidence_score, 0.7)
                
        except (TimeoutError, Exception):
            # Fallback silencieux pour CGI
            pass
        
        # === RECHERCHE BOFIP AVEC TIMEOUT ===
        yield json.dumps({
            "type": "status",
            "message": "üìã Consultation du BOFIP...", 
            "progress": 45
        }) + "\n"
        
        try:
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(3)
            
            similar_bofip_chunks_raw = search_similar_bofip_chunks(query, top_k=4)
            similar_bofip_chunks = [c for c in similar_bofip_chunks_raw if c.get('similarity', 0) >= 0.6][:2]
            signal.alarm(0)
            
            if similar_bofip_chunks:
                all_sources.extend([f"BOFIP: {chunk.get('file', 'Chunk inconnu')}" for chunk in similar_bofip_chunks])
                confidence_score = max(confidence_score, 0.75)
                
        except (TimeoutError, Exception):
            # Fallback silencieux pour BOFIP
            pass

        # === PR√âPARATION DU PROMPT ===
        yield json.dumps({
            "type": "status",
            "message": "ü§ñ Francis analyse votre question...",
            "progress": 65
        }) + "\n"

        # Logique de fallback comme dans get_fiscal_response
        if not similar_cgi_articles and not similar_bofip_chunks:
            query_lower = query.lower()
            
            if any(word in query_lower for word in ['tmi', 'tranche', 'marginal', 'imposition']):
                context = "Les TMI 2025 sont : 0%, 11%, 30%, 41%, 45%"
                all_sources.append("Bar√®me fiscal 2025")
                confidence_score = 0.8
            elif any(word in query_lower for word in ['pea', 'plan', '√©pargne', 'actions']):
                context = "Le PEA permet d'investir en actions europ√©ennes avec exon√©ration fiscale apr√®s 5 ans"
                all_sources.append("Code fiscal - PEA")
                confidence_score = 0.8
            elif any(word in query_lower for word in ['plus-value', 'immobilier', 'r√©sidence principale']):
                context = "La r√©sidence principale est exon√©r√©e de plus-values. Pour les autres biens, abattement selon dur√©e de d√©tention"
                all_sources.append("CGI - Plus-values immobili√®res")
                confidence_score = 0.8
            elif any(word in query_lower for word in ['micro', 'entrepreneur', 'r√©gime']):
                context = "Le r√©gime micro permet de d√©clarer le chiffre d'affaires avec abattement forfaitaire"
                all_sources.append("CGI - R√©gimes fiscaux")
                confidence_score = 0.8
            else:
                context = "En tant qu'expert fiscal, je vais analyser votre situation"
                all_sources.append("Expertise Francis")
                confidence_score = 0.6
                
            # Prompt simplifi√©
            prompt = f"""Tu es Francis, assistant fiscal expert de Fiscal.ia.

Question: {query}

Contexte fiscal disponible: {context}

Historique r√©cent: {conversation_history[-2:] if conversation_history else "Nouvelle conversation"}

R√©ponds de mani√®re claire et pratique en tant qu'expert fiscal, sans formatage markdown.
Si tu n'as pas assez d'informations pr√©cises, explique ce qu'il faudrait savoir pour donner une r√©ponse compl√®te."""
        else:
            # Prompt complet avec RAG
            prompt = create_prompt(query, similar_cgi_articles, similar_bofip_chunks, conversation_history)

        yield json.dumps({
            "type": "status",
            "message": "‚úçÔ∏è G√©n√©ration de la r√©ponse...",
            "progress": 80
        }) + "\n"

        # === APPEL MISTRAL STREAMING AVEC TIMEOUT ===
        try:
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(12)  # Timeout plus long pour le streaming
            
            messages = [ChatMessage(role="user", content=prompt)]
            
            chat_response = client.chat(
                model="mistral-large-latest",
                messages=messages,
                temperature=0.5,
                max_tokens=1200,
                stream=True
            )
            
            # Commencer la r√©ponse
            yield json.dumps({
                "type": "start_response",
                "message": "Francis commence sa r√©ponse:",
                "progress": 90
            }) + "\n"
            
            full_answer = ""
            
            # Streamer la r√©ponse chunk par chunk
            for chunk in chat_response:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    full_answer += content
                    
                    yield json.dumps({
                        "type": "content",
                        "chunk": content
                    }) + "\n"
            
            signal.alarm(0)  # Annuler le timeout
            
            # Finaliser avec les m√©tadonn√©es
            yield json.dumps({
                "type": "complete",
                "sources": list(set(all_sources)),
                "confidence": confidence_score,
                "total_length": len(full_answer),
                "message": "‚úÖ R√©ponse compl√®te!"
            }) + "\n"
            
        except (TimeoutError, Exception) as e:
            # Fallback en cas de timeout de l'API Mistral
            yield json.dumps({
                "type": "start_response",
                "message": "Francis r√©pond en mode fallback:",
                "progress": 90
            }) + "\n"
            
            # R√©ponses de fallback intelligentes
            fallback_responses = {
                'tmi': "Votre TMI d√©pend de vos revenus 2025. Les tranches sont : 0% jusqu'√† 11 294‚Ç¨, 11% jusqu'√† 28 797‚Ç¨, 30% jusqu'√† 82 341‚Ç¨, 41% jusqu'√† 177 106‚Ç¨, 45% au-del√†. Voulez-vous que je calcule votre TMI exacte ?",
                'pea': "Le PEA vous permet d'investir jusqu'√† 150 000‚Ç¨ en actions europ√©ennes. Les gains sont exon√©r√©s d'imp√¥t apr√®s 5 ans de d√©tention. Souhaitez-vous des pr√©cisions sur les conditions ?",
                'plus-value': "Les plus-values immobili√®res b√©n√©ficient d'abattements selon la dur√©e de d√©tention. Votre r√©sidence principale est totalement exon√©r√©e. Avez-vous un projet de vente sp√©cifique ?",
                'micro': "Le r√©gime micro vous permet de d√©clarer uniquement votre chiffre d'affaires avec un abattement forfaitaire. Quel est votre domaine d'activit√© pour vous conseiller pr√©cis√©ment ?",
            }
            
            query_lower = query.lower()
            fallback_answer = None
            
            for keyword, response in fallback_responses.items():
                if keyword in query_lower:
                    fallback_answer = response
                    all_sources = ["Expertise Francis"]
                    confidence_score = 0.7
                    break
            
            if not fallback_answer:
                fallback_answer = f"Je vais analyser votre question fiscale sur '{query}'. Pour vous donner une r√©ponse pr√©cise et personnalis√©e, pouvez-vous me pr√©ciser votre situation (salari√©, entrepreneur, investisseur) et votre objectif ?"
                all_sources = ["Expert Francis"]
                confidence_score = 0.6
            
            # Simuler le streaming de la r√©ponse de fallback
            words = fallback_answer.split()
            for i, word in enumerate(words):
                if i == 0:
                    content = word
                else:
                    content = " " + word
                    
                yield json.dumps({
                    "type": "content",
                    "chunk": content
                }) + "\n"
                
                # Petit d√©lai pour simuler le streaming
                time.sleep(0.05)
            
            yield json.dumps({
                "type": "complete",
                "sources": all_sources,
                "confidence": confidence_score,
                "total_length": len(fallback_answer),
                "message": "‚úÖ R√©ponse de fallback fournie!"
            }) + "\n"
        
    except Exception as e:
        yield json.dumps({
            "type": "error",
            "message": f"Erreur technique: {str(e)[:100]}"
        }) + "\n"

def main():
    print("Assistant Fiscal Expert (tapez 'quit' pour quitter)")
    print("=" * 50)
    
    while True:
        query = input("\nVotre question : ").strip()
        if query.lower() == 'quit':
            break
            
        try:
            # La fonction main est surtout pour le test en CLI, 
            # la structure de retour de get_fiscal_response a chang√©.
            answer, sources, confidence = get_fiscal_response(query)
            print("\nR√©ponse de Francis:")
            print("-" * 50)
            print(answer)
            print("-" * 50)
            if sources:
                print(f"Sources potentielles: {', '.join(sources)}")
            print(f"Confiance (estimation): {confidence:.2f}")
            print("-" * 50)
        except Exception as e:
            print(f"Erreur : {str(e)}")
        
        # time.sleep(1) # D√©lai entre les requ√™tes, peut-√™tre moins n√©cessaire en CLI

if __name__ == "__main__":
    if not MISTRAL_API_KEY:
        exit(1)
    if not client:
        exit(1)
    main() 